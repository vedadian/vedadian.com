حالا که در دو قسمت قبلی ([اول](/learnable-models-this-understanding-computer-how-does-it-learn) و [دوم](/learnable-models-straight-line-the-simple-righteous-function)) از نقطه‌ی آغاز به استفاده عملیاتی از ساده‌ترین تابع آموزش‌پذیر رسیدیم، بهتر است با شتاب بسیار زیاد پرونده‌ی این موضوع را ببندیم؛ زیرا اولاً سریال‌های طولانی حوصله‌ی آدم را سر می‌برد و ثانیاً حتی اگر یک کتاب هم اینجا در رابطه با توابع آموزش‌پذیر بنویسم، مباحث تمام نمی‌شود. البته از همین حالا اعتراف کنم که باز هم سراغ همین موضوع و به دفعات خواهم آمد -- البته در آینده -- چون یکی از علایق من هوش مصنوعی است که این مدل‌ها زیربنای آن است.

شاید بد نباشد پیش از شروع صحبت به این اشاره کنم که چرا خطوط جداکننده‌ی قبیله‌ها را در پست قبلی، خطوط پشتیبان نام گذاشتم. خب، در اصل من این نام را انتخاب نکردم، پیش از من دوستان دیگری [Support Vector Machine](https://fa.wikipedia.org/wiki/%D9%85%D8%A7%D8%B4%DB%8C%D9%86_%D8%A8%D8%B1%D8%AF%D8%A7%D8%B1_%D9%BE%D8%B4%D8%AA%DB%8C%D8%A8%D8%A7%D9%86%DB%8C) را به ماشین بردار پشتیبان ترجمه کرده‌اند که یکی از انواع مدل آموزش‌پذیر است و بر مبنای همان چیزی که در پست قبلی با کمی تفصیل دیدیم کار می‌کند. من هم اسم بهتری برای این خط‌ها نمی‌شناسم.

## مدل‌های آموزش‌پذیر واقعی
اما اصل مطلب. توابعی که در عمل برای استفاده برگزیده می‌شوند، معمولاً خیلی خیلی پیچیده‌تر از یک خط راست هستند. توابعی که تعداد پارامترهای آن‌ها از مرتبه‌ی ده میلیون است. برای مثال در شبکه‌های عصبی عمیقی که برای ترجمه‌ی ماشینی مورد استفاده قرار می‌گیرند، بیش از ۸ میلیون پارامتر وجود دارد (این [مقاله](https://arxiv.org/pdf/1409.0473.pdf) را ببینید.)

این تفاوت فاحش تعداد پارامترها، هرچند مفاهیم اصلی را تغییر نمی‌دهد، جزئیات اجرایی فنی را به شدت دستخوش تغییر می‌کند. مثلاً در مواردی که توابع ساده‌ای داریم که تعداد کمی پارامتر دارد، نقاط زینی و کمینه‌های محلی در دامنه‌ی تابع خیلی خیلی کم‌تر هستند و به همین جهت الگوریتم‌های بهینه سازی با احتمال بالاتری به نقطه‌ی بهینه می‌رسند. در عین حال، روش‌های بهینه‌سازی که مبتنی بر مشتقات مرتبه‌ی دوم عمل می‌کنند و سرعت همگرایی بالایی دارند، در مورد توابع با پارامتر‌های خیلی زیاد، حافظه‌ی بسیار بیشتری طلب می‌کنند. اگر اصطلاحاتی که به کار بردم خیلی برایتان مأنوس نیست، اصلاً نگران نباشید، خلاصه‌ی آنچه گفتم این است بحث بهینه‌سازی و یافتن پارامترهای بهینه بسیار دشوارتر می‌شود.

اما دنیای ما نسبت به اولین باری که روش [اولیه-ثانویه](https://fa.wikipedia.org/wiki/%D8%AF%D9%88%DA%AF%D8%A7%D9%86%DA%AF%DB%8C_(%D8%A8%D9%87%DB%8C%D9%86%D9%87%E2%80%8C%D8%B3%D8%A7%D8%B2%DB%8C)) در عمل مورد استفاده قرار گرفت پیشرفت زیادی کرده است. پیش از این سوای نیاز به دانش حداقلی برای محاسبه‌ی مشتق و مشتق‌مرتبه‌ی دوم توابع پیچیده، نیاز به منابع پردازشی دور از دسترس عموم بود تا آموزش توابع واقعی عملاً امکان‌پذیر شود. حالا اما با داشتن یک GPU مناسب به قیمت حدود ۱.۵ میلیون تومان، می‌توانید از کتابخانه‌های نرم‌افزاری جدیدی که خودشان همه‌ی کار مربوط به محاسبه‌ی مشتق و بهینه‌سازی را برایتان انجام می‌دهند استفاده کنید و برنامه‌های هیجان انگیز بنویسید.

اگر به مباحث مربوط به هوش مصنوعی علاقه‌مند هستید، آشنایی با [PyTorch](http://pytorch.org/) را به شدت توصیه می‌کنم. این کتابخانه‌ی غنی که برای زبان پایتون نوشته شده، می‌تواند سرعت بسیار زیادی به تحقیقات در مورد هوش مصنوعی بدهد. وقتی مدلی را با استفاده از این کتابخانه تعریف می‌کنید، هم کار مشتق‌گیری و هم کار بهینه‌سازی توسط ابزارهایی که در اختیارتان قرار می‌دهد -- به سادگی -- قابل انجام است. علاوه بر این، اجرای آموزش و آزمایش مدل‌هایی که طراحی می‌کنید با اجرای تعداد انگشت‌شماری دستور بر روی GPU اتفاق خواهد افتاد. چیزی که پیش از این نیازمند دانش بیشتر و کار بسیار بسیار بیشتری بود.

چیز دیگری که جالب توجه است، مثال‌های کاربردی و خوبی است که تیم توسعه دهنده با استفاده از همین کتابخانه نوشته و در اختیار عموم قرار داده‌اند. این مثال‌ها از [گیت‌هاب](https://github.com/pytorch/examples) قابل دریافت هستند. پیش از تلاش برای اجرای مثال‌ها، یادتان نرود که خود PyTorch را نصب کنید! دستورات مربوط به نصب PyTorch در صفحه‌ی اول [سایت](http://pytorch.org/) در قسمت «Run this command:» قابل مشاهده‌اند.

برای اینکه انگیزه‌ی بیشتری برای سر زدن به مثال‌ها داشته باشید، عناوینی از میانشان که برای خودم جذاب‌تر بودند را همینجا لیست می‌کنم:
1. Super Resolution (یک جورهایی افزایش کیفیت تصویر)
2. Language Model (ابزاری برای تولید متن به سبکی خاص و امتیازدهی به کیفیت جملات)
3. MNIST (بازشناسی ارقام دستنویس انگلیسی)

یک مثال با نمک دیگر هم انتقال سبک تصویر است:
![انتقال سبک تصویر](/img/neuralstyle.png)
که می‌توانید هم کد منبع و هم توضیحاتش را [اینجا](http://pytorch.org/tutorials/advanced/neural_style_tutorial.html) پیدا کنید.

## چرا سریال مدل‌های آموزش‌پذیر بوجود آمد
در طول این چند سالی که در زمینه‌های مرتبط با هوش مصنوعی کار و تحقیق می‌کردم، خیلی پیش آمده بود که زاویه‌ی نگاه دوستان و همکارانم به یادگیری ماشین را نمی‌پسندیدم. منظورم این است که هنگامی که مسائل از فرم رایج آن خارج می‌شدند -- مثلاً آموزش به شیوه‌ای جدید مطرح می‌شد و یا تابعی جدید پیشنهاد می‌شد و یا اصلاً ارتباط بین یادگیری مانیفلد و شبکه‌های عصبی مصنوعی مورد اشاره قرار می‌گرفت -- آن زاویه‌ی نگاه مانع فهم عمیق می‌شد (لااقل من اینطوی فکر می‌کردم!)

برای همین سعی کردم فارغ از جزئیات فنی، چیزی بنویسم که با شفافیت بیشتری شیوه‌ی یادگیری ماشین را نشان دهد. اینکه یادگیری ماشین همان یافتن نقطه‌ی کمینه‌ی یک تابع ریاضی است که نوع ساده‌اش را در ریاضیات دبیرستانی هم می‌بینیم. اینکه اگر شبکه‌های عصبی را با گراف‌هایی زیبا که یادآور ارتباطات نرون‌های داخل مغز است نشان می‌دهیم، یادمان نرود که روابط ریاضی مربوط به ارتباط دو لایه‌ی آن به سادگی $y=\sigma\left(Wx+b\right)$ است.